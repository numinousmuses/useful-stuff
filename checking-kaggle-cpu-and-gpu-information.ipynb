{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/joshuaokolo/checking-kaggle-cpu-and-gpu-information?scriptVersionId=103654677\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-19T01:09:19.816498Z","iopub.execute_input":"2022-08-19T01:09:19.816966Z","iopub.status.idle":"2022-08-19T01:09:19.848936Z","shell.execute_reply.started":"2022-08-19T01:09:19.816875Z","shell.execute_reply":"2022-08-19T01:09:19.847868Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\n\ndevice_lib.list_local_devices()","metadata":{"execution":{"iopub.status.busy":"2022-08-19T01:09:22.718446Z","iopub.execute_input":"2022-08-19T01:09:22.718818Z","iopub.status.idle":"2022-08-19T01:09:29.558886Z","shell.execute_reply.started":"2022-08-19T01:09:22.718772Z","shell.execute_reply":"2022-08-19T01:09:29.557858Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2022-08-19 01:09:27.166412: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 17767672435862830991,\n name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 16152002560\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 11591061854719963039\n physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"},"metadata":{}},{"name":"stderr","text":"2022-08-19 01:09:27.233729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-19 01:09:27.350044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-19 01:09:27.350865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-19 01:09:29.543239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-19 01:09:29.544107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-19 01:09:29.544773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-19 01:09:29.545378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')","metadata":{"execution":{"iopub.status.busy":"2022-08-19T01:09:42.147533Z","iopub.execute_input":"2022-08-19T01:09:42.148303Z","iopub.status.idle":"2022-08-19T01:09:46.022765Z","shell.execute_reply.started":"2022-08-19T01:09:42.148258Z","shell.execute_reply":"2022-08-19T01:09:46.019631Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Using device: cuda\n\nTesla P100-PCIE-16GB\nMemory Usage:\nAllocated: 0.0 GB\nCached:    0.0 GB\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:386: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n  FutureWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"from pynvml import *\nnvmlInit()\nh = nvmlDeviceGetHandleByIndex(0)\ninfo = nvmlDeviceGetMemoryInfo(h)\nprint(f'total    : {info.total/1073741824} GB')\nprint(f'free     : {info.free/1073741824} GB')\nprint(f'used     : {info.used/1073741824} GB')","metadata":{"execution":{"iopub.status.busy":"2022-08-19T01:10:01.746503Z","iopub.execute_input":"2022-08-19T01:10:01.747138Z","iopub.status.idle":"2022-08-19T01:10:01.772521Z","shell.execute_reply.started":"2022-08-19T01:10:01.747098Z","shell.execute_reply":"2022-08-19T01:10:01.771324Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"total    : 15.8992919921875 GB\nfree     : 15.183349609375 GB\nused     : 0.7159423828125 GB\n","output_type":"stream"}]}]}